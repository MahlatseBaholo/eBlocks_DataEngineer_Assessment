import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder \
        .appName("Schema Test") \
        .config("spark.driver.extraClassPath", "/extra-jars/mysql-connector-j-8.0.33.jar") \
        .config("spark.executor.extraClassPath", "/extra-jars/mysql-connector-j-8.0.33.jar") \
        .config("spark.jars", "/extra-jars/mysql-connector-j-8.0.33.jar") \
        .getOrCreate()

def test_schema_fields(spark):
    df = spark.read \
        .format("jdbc") \
        .option("url", "jdbc:mysql://mysql:3306/northwind") \
        .option("dbtable", "customers") \
        .option("user", "user") \
        .option("password", "rootpass") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .load()

    # Check that the required columns are present
    required_fields = {"id", "last_name", "address2"}
    actual_fields = set(df.columns)
    assert required_fields.issubset(actual_fields), f"Missing columns: {required_fields - actual_fields}"

    # Check that none of the important columns have null values
    for column in required_fields:
        null_count = df.filter(col(column).isNull()).count()
        assert null_count == 0, f"Column '{column}' contains {null_count} null values"
